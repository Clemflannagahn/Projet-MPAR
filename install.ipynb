{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import graphviz as gv\n",
    "import imageio\n",
    "import os\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "class States():\n",
    "    def __init__(self, state,transitions_with_action, transitions_without_action, resolution = False):\n",
    "        \"\"\"\n",
    "        Initialisation des états de la chaine de markov\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "        state : str\n",
    "            Nom de l'état (S0...)\n",
    "        reward : int\n",
    "            Récompense de l'état\n",
    "        transitions_with_action : dict\n",
    "            Dictionnaire de l'ensemble des transitions avec action\n",
    "        transitions_without_action : dict\n",
    "            Dictionnaire de l'ensemble des transitions sans action\n",
    "        resolution : bool\n",
    "            Booléen qui permet de savoir si on veut résoudre les problèmes détéctés automatiquement si possible\n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "        self.reward = 0 # récompense de l'état\n",
    "        self.transitions_without_action = {}\n",
    "        self.transitions_with_action = {}\n",
    "\n",
    "        sans_action = True # on crée ce booléen pour vérifier qu'il n'y a pas de transition avec action ensuite\n",
    "        for transition in transitions_without_action:\n",
    "            # on parcours l'ensemble des transitions\n",
    "            transi_active = transitions_without_action[transition]\n",
    "            if transi_active[\"from\"]==self.state: # on vérifie si l'état de départ est l'état actif\n",
    "                sans_action = False\n",
    "                self.transitions_without_action[\"targets\"] = transi_active[\"targets\"]\n",
    "                self.transitions_without_action[\"weights\"] = transi_active[\"weights\"]\n",
    "\n",
    "        for transition in transitions_with_action:\n",
    "            transi_active = transitions_with_action[transition]\n",
    "            if transi_active[\"from\"]==self.state and sans_action == True:\n",
    "                # les clés dans transitions_with_action sont les actions et les valeurs des dict des états cibles et leurs poids\n",
    "                self.transitions_with_action[transi_active[\"action\"]] = {\"targets\" : transi_active[\"targets\"], \"weights\" : transi_active[\"weights\"]}\n",
    "            elif transi_active[\"from\"]==self.state and sans_action == False:\n",
    "                print(\"\\nWarning : l'état\", self.state, \"comporte des transitions avec et sans action\")\n",
    "                if resolution == True: # On choisit pour résoudre le problème de supprimer les transitions avec action de l'état, pour ça on ne rajoute pas la transition dans self.transitions_with_action\n",
    "                    print(\"Le problème a été résolu automatiquement : une transition avec action de l'état\", self.state, \"a été supprimée\")\n",
    "                    pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"State : {self.state} \\n State reward {self.reward} \\n Transitions without action : {self.transitions_without_action} \\n Transitions with action : {self.transitions_with_action}\")\n",
    "\n",
    "\n",
    "class markov():\n",
    "    def __init__(self, fichier_mdp):\n",
    "        \"\"\"\n",
    "        Initialisation de la chaine de markov\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        fichier_mdp : str\n",
    "            Nom du fichier contenant la modélisation de la chaine de markov\n",
    "        \"\"\"\n",
    "        ! python mdp_lecture/mdp.py < {fichier_mdp} # On lance le fichier mdp.py avec la modélisation voulue\n",
    "        # On lit le .pickle des données que l'on souhaitent récupérer\n",
    "        def read_list(nom_liste):\n",
    "            # for reading also binary mode is important\n",
    "            with open(nom_liste, 'rb') as fp:\n",
    "                liste = pickle.load(fp)\n",
    "                return liste\n",
    "\n",
    "        L = read_list(\"liste_donnees\")\n",
    "        # print('Données récupérées', L)\n",
    "        states, actions, transitions_with_action, transitions_without_action = L['States'], L['Actions'], L['Transitions_with_action'], L['Transitions_without_action']\n",
    "        print(\"-\"*50 + \"\\n\")\n",
    "\n",
    "        # Pour savoir si on veut résoudre les problèmes détectés automatiquement\n",
    "\n",
    "        print(\"\\n Souhaitez-vous que les problèmes détéctés prochainement soient résolus automatiquement si cela est possible? (y/n)\")\n",
    "        time.sleep(0.5)\n",
    "        reponse_resolution_pb = input()\n",
    "        if reponse_resolution_pb == \"y\":\n",
    "            resolution = True\n",
    "        else:\n",
    "            resolution = False\n",
    "\n",
    "\n",
    "        # on peut alors initialiser les éléments de la chaine de markov\n",
    "        self.states = {}\n",
    "        self.liste_states = states\n",
    "        self.actions = actions\n",
    "        self.transitions_with_action = transitions_with_action\n",
    "        self.transitions_without_action = transitions_without_action\n",
    "\n",
    "        for state in states:\n",
    "            self.states[f\"{state}\"] = States(state, transitions_with_action, transitions_without_action, resolution)\n",
    "        \n",
    "        # On vérifie que la chaine de markov est correctement définie et on résout les problèmes si possibles et si souhaité\n",
    "        markov.parsing(self, states, resolution) # on vérifie qu'on a pas de problèmes de parsing\n",
    "\n",
    "        print(\"Souhaitez-vous ajouter des récompenses aux états ? (y/n)\")\n",
    "        time.sleep(0.5)\n",
    "        reponse = input()\n",
    "        if reponse == \"y\":\n",
    "            for state in self.states :\n",
    "                etat = self.states[state]\n",
    "                reward = markov.affichage_etat(self, etat, reward = True)\n",
    "                etat.reward = float(reward)\n",
    "        \n",
    "        print(\"Que souhaitez-vous faire ?\")\n",
    "        print(\"1. lancer un parcours\")\n",
    "        print(\"2. model checking d'un eventually\")\n",
    "        print(\"3. SMC quantitatif\")\n",
    "        time.sleep(0.5)\n",
    "        reponse = input()\n",
    "        while reponse not in [\"1\", \"2\", \"3\"]:\n",
    "            reponse = input()\n",
    "        if reponse == \"1\":\n",
    "            markov.parcours(self)\n",
    "        elif reponse == \"2\":\n",
    "            print(\"Quel sont les états cibles ? Veuillez les rentrer un par un, et taper 'fin' quand vous avez fini\")\n",
    "            time.sleep(0.5)\n",
    "            etat = input()\n",
    "            etats_cibles = []\n",
    "            while etat != \"fin\":\n",
    "                if etat not in self.liste_states:\n",
    "                    print(\"L'état\", etat, \"n'existe pas\")\n",
    "                    etat = input()\n",
    "                else :\n",
    "                    etats_cibles.append(etat)\n",
    "                    etat = input()\n",
    "            markov.eventually_check_DTMC(self, etats_cibles)\n",
    "        elif reponse == \"3\" :\n",
    "            if self.transitions_with_action == {}:\n",
    "                markov.smc_quantitatif(self)\n",
    "            else :\n",
    "                print(\"SMC quantitatif impossible car la chaine de markov n'est pas déterministe\")\n",
    "                return(False) # changer pour pouvoir lancer autre chose\n",
    "\n",
    "\n",
    "        \n",
    "    def parsing(self, states, resolution):\n",
    "        \"\"\"\n",
    "        On réalise différents tests pour vérifier que la chaine de markov (ou la MDP) est correctement définie\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        states : list\n",
    "            Liste des états de la chaines (avec possiblement des doublons)\n",
    "        resolution : bool\n",
    "            Booléen qui permet de savoir si on veut résoudre les problèmes détéctés automatiquement si possible\n",
    "        \"\"\"\n",
    "        # On concatène les transitions avec et sans action pour plus de simplicité pour les test\n",
    "        Warning = False # passe à True si on a un warning\n",
    "\n",
    "        all_transitions = []\n",
    "        transitions_without_action = []\n",
    "        transitions_with_action = []\n",
    "        for elem in self.transitions_without_action:\n",
    "            transitions_without_action.append(self.transitions_without_action[elem])\n",
    "            all_transitions.append(self.transitions_without_action[elem])\n",
    "        for elem in self.transitions_with_action:\n",
    "            transitions_with_action.append(self.transitions_with_action[elem])\n",
    "            all_transitions.append(self.transitions_with_action[elem])\n",
    "\n",
    "        # On vérifie que les états déclarés sont utilisés et qu'un état utilisé dans des transitions est déclaré\n",
    "        used_states = []\n",
    "        for transitions in all_transitions:\n",
    "            if transitions[\"from\"] not in self.states:\n",
    "                print(f\"Warning : l'état {transitions['from']} est utilisé dans une transition mais n'est pas déclaré\")\n",
    "                Warning = True\n",
    "                if resolution:\n",
    "                    # On rajoute l'état manquant\n",
    "                    self.states[transitions[\"from\"]] = States(transitions[\"from\"], self.transitions_with_action, self.transitions_without_action)\n",
    "\n",
    "            used_states.append(transitions[\"from\"])\n",
    "            used_states.append(transitions[\"targets\"])\n",
    "            for target in transitions[\"targets\"]:\n",
    "                if target not in self.states:\n",
    "                    print(f\"Warning : l'état {target} est utilisé dans une transition mais n'est pas déclaré\")\n",
    "                    Warning = True\n",
    "                    if resolution:\n",
    "                        # On rajoute l'état manquant\n",
    "                        print(\"Rajout de l'état manquant\")\n",
    "                        self.states[target] = States(target, self.transitions_with_action, self.transitions_without_action)\n",
    "        for state in self.states:\n",
    "            if state not in used_states:\n",
    "                print(f\"Warning : l'état {state} est déclaré mais n'est pas utilisé\")\n",
    "                Warning = True\n",
    "                if resolution:\n",
    "                    # On rajoute une transition de l'état vers lui même pour éviter les erreurs\n",
    "                    print(\"Rajout d'une transition de l'état vers lui même\")\n",
    "                    self.transitions_without_action[f\"{state}\"] = {'from': f'{state}', 'targets': [f\"{state}\"], 'weights': [10]}\n",
    "                    self.states[f\"{state}\"].transitions_without_action = self.transitions_without_action[f\"{state}\"]\n",
    "\n",
    "        # On vérifie qu'un état n'est pas déclaré plusieurs fois\n",
    "        if len(states) != len(self.states): # self.states supprime automatiquement les doublons\n",
    "            print(\"Warning : un état est déclaré plusieurs fois\")\n",
    "            Warning = True\n",
    "\n",
    "        # On vérifie qu'un état possède bien une transition de sortie\n",
    "        etats_sans_sortie = []\n",
    "        dict_etats = self.states\n",
    "        bool_etat_sans_sortie = False\n",
    "        for state in dict_etats:\n",
    "            if dict_etats[state].transitions_without_action == {} and dict_etats[state].transitions_with_action == {}:\n",
    "                etats_sans_sortie.append(state)\n",
    "                bool_etat_sans_sortie = True\n",
    "                if resolution:\n",
    "                    # On rajoute une transition de l'état vers lui même pour pour lui créer un état de sortie\n",
    "                    self.transitions_without_action[f\"{state}\"] = {'from': f'{state}', 'targets': [f\"{state}\"], 'weights': [10]}\n",
    "                    self.states[f\"{state}\"].transitions_without_action = {'from': f'{state}', 'targets': [f\"{state}\"], 'weights': [10]}\n",
    "\n",
    "        if bool_etat_sans_sortie == True:\n",
    "            print(\"Warning : les états suivants n'ont pas de transition de sortie : \", etats_sans_sortie)\n",
    "            print(f\"Rajout d'une transition des états {etats_sans_sortie} vers eux mêmes\")\n",
    "\n",
    "        # On vérifie que les actions ne sont pas déclarées plusieurs fois\n",
    "        actions_uniques = set(self.actions)\n",
    "        if len(actions_uniques) != len(self.actions):\n",
    "            print(\"Warning : une action est déclarée plusieurs fois\")\n",
    "            Warning = True\n",
    "            if resolution:\n",
    "                print(\"Suppressions automatiques des actions déclarées plusieurs fois\")\n",
    "    \n",
    "        # On vérifie qu'une action déclarée est utilisée\n",
    "        actions_utilisés = []\n",
    "        for transitions in transitions_with_action:\n",
    "            actions_utilisés.append(transitions[\"action\"])\n",
    "        actions_utilisés = set(actions_utilisés) # on enlève les doublons\n",
    "        if len(actions_utilisés) != len(self.actions) and self.actions != [\"None\"]: # On met None de base s'il n'y a pas d'action\n",
    "            print(\"Warning : une action est déclarée mais n'est pas utilisée\")\n",
    "            Warning = True\n",
    "            if resolution:\n",
    "                print(\"Suppression des actions non utilisées\")\n",
    "                # On supprime les actions non utilisées\n",
    "                for action in self.actions:\n",
    "                    if action not in actions_utilisés:\n",
    "                        self.actions.remove(action)\n",
    "\n",
    "        # On vérifie que les actions utilisées sont déclarées\n",
    "        for action in actions_utilisés:\n",
    "            if action not in self.actions:\n",
    "                print(f\"Warning : l'action {action} est utilisée mais n'est pas déclarée\")\n",
    "                Warning = True\n",
    "                if resolution:\n",
    "                    print(\"Rajout de l'action manquante\")\n",
    "                    # On rajoute l'action manquante\n",
    "                    if self.actions == [\"None\"]:\n",
    "                        self.actions = [action]\n",
    "                    else:\n",
    "                        self.actions.append(action)\n",
    "\n",
    "        if Warning :\n",
    "            print(\"\\n Écrire ok pour contiuer\")\n",
    "            print(\"-\"*25 + \"\\n\")\n",
    "            while True:\n",
    "                rep = input()\n",
    "                if rep == \"ok\":\n",
    "                    break\n",
    "            \n",
    "    def affichage_etat(self,etat_actif,choix = False, reward = False):\n",
    "        \"\"\"\n",
    "        Affiche proprement l'état actif de la chaine de markov et permet de choisir une action le cas échéant\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        etat_actif : état de la chaine de markov\n",
    "            Etat actif de la chaine de markov\n",
    "        choix : bool\n",
    "            Si on doit afficher des choix pour des actions\n",
    "        \"\"\"\n",
    "        nom = etat_actif.state\n",
    "        print(\"-\"*50)\n",
    "        chaine_nom = f\"| Etat actif : {nom}\"\n",
    "        l = len(chaine_nom)\n",
    "        print(chaine_nom + \" \"*(49-l) + \"|\")\n",
    "        if choix :\n",
    "            chaine_choix = f\"| choix possibles pour l'état {etat_actif.state}:\"\n",
    "            l = len(chaine_choix)\n",
    "            print(chaine_choix + \" \"*(49-l) + \"|\")\n",
    "\n",
    "            chaine_transi = f\"| {list(etat_actif.transitions_with_action.keys())}\"\n",
    "            l = len(chaine_transi)\n",
    "            print(chaine_transi + \" \"*(49-l) + \"|\")\n",
    "\n",
    "            chaine_action = f\"| choix de l'action  :\"\n",
    "            l = len(chaine_action)\n",
    "            print(chaine_action + \" \"*(49-l) + \"|\")\n",
    "            time.sleep(0.5)\n",
    "            choix = input()\n",
    "            while choix not in list(etat_actif.transitions_with_action.keys()): # on vérifie que l'action choisie est valide\n",
    "                choix = input()\n",
    "                if choix == \"stop\": # permet d'arrêter le programme à la main\n",
    "                    break\n",
    "            chaine_choix = f\"| Vous avez choisi l'action {choix}\"\n",
    "            l = len(chaine_choix)\n",
    "            print(chaine_choix + \" \"*(49-l) + \"|\")\n",
    "\n",
    "        elif reward:\n",
    "            chaine_choix = f\"| Récompense pour l'état {etat_actif.state}:\"\n",
    "            l = len(chaine_choix)\n",
    "            print(chaine_choix + \" \"*(49-l) + \"|\")\n",
    "            time.sleep(0.5)\n",
    "            choix = input()\n",
    "            chaine_rec = f\"| Vous avez choisi : {choix}\"\n",
    "            l = len(chaine_rec)\n",
    "            print(chaine_rec + \" \"*(49-l) + \"|\")\n",
    "\n",
    "            \n",
    "        print(\"-\"*50)\n",
    "        print(\"\\n\")   \n",
    "        return choix    \n",
    "\n",
    "    def parcours(self, file_name=\"visu_parcours\"): # on parcours la chaine (en faisant N étapes)\n",
    "        \"\"\" \n",
    "        On parcours la chaine en faisant N étapes\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "        file_name : str\n",
    "            Nom du fichier de sauvegarde de la vidéo\n",
    "        \"\"\"\n",
    "        positionnel = False\n",
    "        n_pos = False\n",
    "        without_action = False\n",
    "        \n",
    "        print(\"Quel mode de parcours voulez-vous ?\")\n",
    "        print(\"1 : sans actions\")\n",
    "        print(\"2 : avec adversaire positionnel\")\n",
    "        print(\"3 : avec adversaire non positionnel\")\n",
    "        time.sleep(0.5)\n",
    "        choix_parcours = input()\n",
    "        while choix_parcours not in [\"1\",\"2\",\"3\"]:\n",
    "            choix_parcours = input()\n",
    "        if choix_parcours == \"1\":\n",
    "            without_action = True\n",
    "        elif choix_parcours == \"2\":\n",
    "            positionnel = True\n",
    "        elif choix_parcours == \"3\":\n",
    "            n_pos = True\n",
    "        \n",
    "        print(\"Combien d'étapes voulez-vous faire ?\")\n",
    "        time.sleep(0.5)\n",
    "        N = int(input())\n",
    "\n",
    "        etat_initial = self.liste_states[0] #état initial : premier élément\n",
    "        etat_actif = self.states[etat_initial] # on prend l'objet correspondant à l'état initial\n",
    "        reward_total = 0\n",
    "        \n",
    "        self.afficher(etat = etat_initial, file_name='image0')\n",
    "        images = []\n",
    "        images.append(imageio.imread('image0'+'.png'))\n",
    "\n",
    "        if without_action:\n",
    "        # On vérifie qu'on ne choisit pas le mode \"sans actions\" alors qu'il y en a\n",
    "            for state in self.states:\n",
    "                if len(self.states[state].transitions_with_action) != 0:\n",
    "                    print(f\"Error : il y a des transitions avec actions dans l'état {state}, il faut choisir le mode avec actions\")\n",
    "                    without_action = False\n",
    "                    print(\"Voulez-vous changer de mode ? (y/n, si non, le programme s'arrête)\")\n",
    "                    time.sleep(0.5)\n",
    "                    choix = input()\n",
    "                    while choix not in [\"y\",\"n\"]:\n",
    "                        choix = input()\n",
    "                    if choix == \"y\":\n",
    "                        print(\"Quel mode de parcours voulez-vous ?\")\n",
    "                        print(\"1 : avec adversaire positionnel\")\n",
    "                        print(\"2 : avec adversaire non positionnel\")\n",
    "                        time.sleep(0.5)\n",
    "                        choix_parcours = input()\n",
    "                        while choix_parcours not in [\"1\",\"2\"]:\n",
    "                            choix_parcours = input()\n",
    "                        if choix_parcours == \"1\":\n",
    "                            positionnel = True\n",
    "                        elif choix_parcours == \"2\":\n",
    "                            n_pos = True\n",
    "                    else :\n",
    "                        return\n",
    "        \n",
    "        if without_action:\n",
    "            for i in range(N):\n",
    "                markov.affichage_etat(self, etat_actif)\n",
    "                # print(etat_actif.state) # on affiche l'état en cours\n",
    "                poids = etat_actif.transitions_without_action[\"weights\"]\n",
    "                poids_total = np.sum(poids)\n",
    "                poids = poids/poids_total # on normalise les poids pour qu'ils soient entre 0 et 1\n",
    "                poids = np.cumsum(poids) # on fait la somme cumulée des poids, afin de pouvoir faire un tirage aléatoire (il ne faut pas que par exemple les deux probas soient de 0.5, il en faut une de 0.5 et l'autre de 1)\n",
    "\n",
    "                choix = random.random() # tirage aléatoire entre 0 et 1\n",
    "\n",
    "                for j in range(len(poids)): # on recherche l'état cible\n",
    "                    if choix <= poids[j]:\n",
    "                        reward_total += etat_actif.reward\n",
    "                        etat_actif = self.states[etat_actif.transitions_without_action[\"targets\"][j]]\n",
    "                        break\n",
    "                self.afficher(etat = etat_actif.state, file_name='image'+str(i+1))\n",
    "                images.append(imageio.imread('image'+str(i+1)+'.png'))\n",
    "            # create gif\n",
    "            imageio.mimsave(file_name+'.gif', images, fps=3)\n",
    "            for i in range(N+1):\n",
    "                os.remove('image'+str(i)+'.png')\n",
    "\n",
    "        elif positionnel == True or n_pos == True: # adversaire positionnel\n",
    "            if positionnel :\n",
    "                print(\"Choix d'un adversaire positionnel\")\n",
    "                adv_pos = {} # contient pour chaque état, le choix de l'adversaire\n",
    "                for state in self.states :\n",
    "                    etat = self.states[state]\n",
    "                    if etat.transitions_with_action != {} : # si l'état possèdes des transitions avec actions\n",
    "                        action = markov.affichage_etat(self, etat, choix = True)\n",
    "                        adv_pos[state] = action\n",
    "\n",
    "            for i in range(N):\n",
    "                if etat_actif.transitions_with_action == {} : # si l'état n'a pas de transitions avec actions :\n",
    "                    markov.affichage_etat(self, etat_actif)\n",
    "                    print(etat_actif)\n",
    "                    poids = etat_actif.transitions_without_action[\"weights\"]\n",
    "\n",
    "                    poids_total = np.sum(poids)\n",
    "                    poids = poids/poids_total # on normalise les poids pour qu'ils soient entre 0 et 1\n",
    "                    poids = np.cumsum(poids) # on fait la somme cumulée des poids, afin de pouvoir faire un tirage aléatoire (il ne faut pas que par exemple les deux probas soient de 0.5, il en faut une de 0.5 et l'autre de 1)\n",
    "\n",
    "                    choix = random.random() # tirage aléatoire entre 0 et 1\n",
    "\n",
    "                    for j in range(len(poids)): # on recherche l'état cible\n",
    "                        if choix <= poids[j]:\n",
    "                            reward_total += etat_actif.reward\n",
    "                            etat_actif = self.states[etat_actif.transitions_without_action[\"targets\"][j]]\n",
    "                            break\n",
    "                else :\n",
    "                    if n_pos == True :\n",
    "                        action_choisie = markov.affichage_etat(self, etat_actif, choix =True)\n",
    "                    else :\n",
    "                        action_choisie = adv_pos[etat_actif.state]\n",
    "                        print(f\"Action choisie : {action_choisie}\")\n",
    "                        markov.affichage_etat(self, etat_actif)\n",
    "                    poids = etat_actif.transitions_with_action[action_choisie][\"weights\"] # on ne prend que les poids de l'action choisie par l'adversaire\n",
    "                    poids_total = np.sum(poids)\n",
    "                    poids = poids/poids_total # on normalise les poids pour qu'ils soient entre 0 et 1\n",
    "                    poids = np.cumsum(poids) # on fait la somme cumulée des poids, afin de pouvoir faire un tirage aléatoire (il ne faut pas que par exemple les deux probas soient de 0.5, il en faut une de 0.5 et l'autre de 1)\n",
    "\n",
    "                    choix = random.random() # tirage aléatoire entre 0 et 1\n",
    "\n",
    "                    for j in range(len(poids)): # on recherche l'état cible\n",
    "                        if choix <= poids[j]:\n",
    "                            reward_total += etat_actif.reward\n",
    "                            etat_actif = self.states[etat_actif.transitions_with_action[action_choisie][\"targets\"][j]]\n",
    "                            break\n",
    "                        \n",
    "                self.afficher(etat = etat_actif.state, file_name='image'+str(i+1))\n",
    "                images.append(imageio.imread('image'+str(i+1)+'.png'))\n",
    "            # create gif\n",
    "            imageio.mimsave(file_name+'.gif', images, fps=3)\n",
    "            for i in range(N+1):\n",
    "                os.remove('image'+str(i)+'.png')\n",
    "            for i in range(N+1):\n",
    "                os.remove('image'+str(i))\n",
    "                \n",
    "        print(f\"Reward total : {reward_total} (si aucun reward n'a été donné, le reward total est de 0)\")\n",
    "        print(\"Le graphique est visible dans le fichier visu_parcours.gif\")\n",
    "\n",
    "    def parcours_SMC(self, length, etat): # on parcours la chaine (en faisant N étapes)\n",
    "        \"\"\" \n",
    "        On parcours la chaine sans les actions, sans afficher le graphe. \n",
    "        \n",
    "        Args:\n",
    "            length (int): nombre max d'itération \n",
    "            etat (str): état d'arrivée\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        etat_initial = self.liste_states[0] #état initial : premier élément\n",
    "        etat_actif = self.states[etat_initial] # on prend l'objet correspondant à l'état initial\n",
    "        reward_total = 0\n",
    "        if etat_actif.state == etat:\n",
    "            return etat\n",
    "                        \n",
    "        for i in range(length):\n",
    "            # print(etat_actif.state) # on affiche l'état en cours\n",
    "            poids = etat_actif.transitions_without_action[\"weights\"]\n",
    "            poids_total = np.sum(poids)\n",
    "            poids = poids/poids_total # on normalise les poids pour qu'ils soient entre 0 et 1\n",
    "            poids = np.cumsum(poids) # on fait la somme cumulée des poids, afin de pouvoir faire un tirage aléatoire (il ne faut pas que par exemple les deux probas soient de 0.5, il en faut une de 0.5 et l'autre de 1)\n",
    "\n",
    "            choix = random.random() # tirage aléatoire entre 0 et 1\n",
    "\n",
    "            for j in range(len(poids)): # on recherche l'état cible\n",
    "                if choix <= poids[j]:\n",
    "                    reward_total += etat_actif.reward\n",
    "                    etat_actif = self.states[etat_actif.transitions_without_action[\"targets\"][j]]\n",
    "                    break\n",
    "            if etat_actif.state == etat:\n",
    "                break\n",
    "            \n",
    "        return etat_actif.state\n",
    "\n",
    "\n",
    "    \n",
    "    def afficher(self, etat = \"default\", file_name = \"graph\"):\n",
    "        \n",
    "        \"\"\"Fonction qui permet de représenter le graphe de la chaine de Markov avec ou sans action. Dans le mode \"default\", le graphe de base est affiché.\n",
    "        Dans le mode \"etat\", on peut choisir un état et il sera mis en évidence en étant de couleur bleue.\n",
    "         \"\"\"\n",
    "    \n",
    "        # On récupère les données\n",
    "        States = self.states\n",
    "        Actions = self.actions\n",
    "        Transitions_with_action = self.transitions_with_action\n",
    "        Transitions_without_action = self.transitions_without_action\n",
    "\n",
    "        # On crée le graphique\n",
    "        G = gv.Digraph(format='png')\n",
    "\n",
    "        if etat == \"default\":\n",
    "            for state in States:\n",
    "                G.node(state)\n",
    "        else:\n",
    "            for state in States:\n",
    "                if state == etat:\n",
    "                    G.node(state, color = 'blue', style = 'filled')\n",
    "                else:\n",
    "                    G.node(state)\n",
    "        \n",
    "        #On ajoute les actions\n",
    "        for actions in Actions:\n",
    "            G.node(actions, shape = 'point')\n",
    "            \n",
    "        for transition in Transitions_with_action:\n",
    "            G.edge(Transitions_with_action[transition]['from'], Transitions_with_action[transition]['action'], label=str(Transitions_with_action[transition]['action']), color = 'red')\n",
    "        \n",
    "\n",
    "        # On ajoute les transitions sans action\n",
    "        for transition in Transitions_without_action:\n",
    "            for i in range(len(Transitions_without_action[transition]['targets'])):\n",
    "                G.edge(Transitions_without_action[transition]['from'],Transitions_without_action[transition]['targets'][i], label = str(Transitions_without_action[transition]['weights'][i])) \n",
    "\n",
    "        # On ajoute les transitions avec action\n",
    "        for transition in Transitions_with_action:\n",
    "            for i in range(len(Transitions_with_action[transition]['targets'])):\n",
    "                G.edge(Transitions_with_action[transition]['action'],Transitions_with_action[transition]['targets'][i], label = str(Transitions_with_action[transition]['weights'][i]))\n",
    "        \n",
    "        G.render(file_name, view=False)\n",
    "    \n",
    "    def simulation_random(self, n_iter, mode_adv = \"random\", file_name = \"simulation_random\" ):\n",
    "        \"\"\" Fonction qui permet de simuler une chaine de Markov avec ou sans action. On peut choisir le nombre d'itération, l'état initial et le mode de l'adversaire.\n",
    "        Pour l'instant, seul le mode \"random\" est disponible.\"\"\"\n",
    "        \n",
    "        current_state = self.liste_states[0] #état initial : premier élément\n",
    "        States = self.states\n",
    "        Actions = self.actions\n",
    "        Transitions_with_action = self.transitions_with_action\n",
    "        Transitions_without_action = self.transitions_without_action\n",
    "        self.afficher(etat = current_state, file_name='image0')\n",
    "        reward_total = 0\n",
    "        images = []\n",
    "        if mode_adv == \"random\":\n",
    "            for i in range(n_iter):\n",
    "                    actions_possibles = []\n",
    "                    for transitions in Transitions_with_action:\n",
    "                        if Transitions_with_action[transitions]['from'] == current_state:\n",
    "                            actions_possibles.append(Transitions_with_action[transitions]['action'])\n",
    "                    if len(actions_possibles) == 0:\n",
    "                        for transitions in Transitions_without_action:\n",
    "                            if Transitions_without_action[transitions]['from'] == current_state:\n",
    "                                poids = Transitions_without_action[transitions]['weights']\n",
    "                                poids_total = np.sum(poids)\n",
    "                                poids = poids/poids_total\n",
    "                                poids = np.cumsum(poids)\n",
    "                                choix = random.random()\n",
    "                                for j in range(len(poids)):\n",
    "                                    if choix <= poids[j]:\n",
    "                                        reward_total += self.states[current_state].reward # ----------\n",
    "                                        current_state = Transitions_without_action[transitions]['targets'][j]\n",
    "                                        break\n",
    "                    else:\n",
    "                        action_choisie = random.choice(actions_possibles)\n",
    "                        for transitions in Transitions_with_action:\n",
    "                            if Transitions_with_action[transitions]['action'] == action_choisie:\n",
    "                                poids = Transitions_with_action[transitions]['weights']\n",
    "                                poids_total = np.sum(poids)\n",
    "                                poids = poids/poids_total\n",
    "                                poids = np.cumsum(poids)\n",
    "                                choix = random.random()\n",
    "                                for j in range(len(poids)):\n",
    "                                    if choix <= poids[j]:\n",
    "                                        reward_total += self.states[current_state].reward # ----------\n",
    "                                        current_state = Transitions_with_action[transitions]['targets'][j]\n",
    "                                        break\n",
    "                                    \n",
    "                    self.afficher(etat = current_state, file_name='image'+str(i+1))\n",
    "                    images.append(imageio.imread('image'+str(i+1)+'.png'))\n",
    "        imageio.mimsave(file_name+'.gif', images,fps=3)\n",
    "        for i in range(n_iter+1):\n",
    "            os.remove('image'+str(i)+'.png')\n",
    "        for i in range(n_iter+1):\n",
    "                os.remove('image'+str(i))\n",
    "\n",
    "        print(f\"Reward total : {reward_total}\")\n",
    "        \n",
    "        return current_state\n",
    "\n",
    "    def eventually_check_DTMC(self, S1):\n",
    "        \"\"\" Model Checking du eventually pour une chaine de Markov sans action.\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "        S1 : list\n",
    "            Liste des états qu'on cherche à atteindre\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        S_int = [x for x in self.liste_states if x not in S1] # on crée la liste des étates S_? (\"int\" pour interrogation)\n",
    "        S0 = [] # on crée la liste des états S_0 qu'on remplira par récurrence\n",
    "        S_int_2 = S_int.copy()\n",
    "\n",
    "        for state in S_int : # Algorithme de recherche en profondeur qu'on applique à chaque état de S_int\n",
    "            pile = []\n",
    "            pile.append(state)\n",
    "            etats_marques = [state]\n",
    "            while pile != []:\n",
    "                etat = pile[-1]\n",
    "                etat_actif = self.states[etat]\n",
    "                if etat in S1 :\n",
    "                    break\n",
    "                etats_faisables = [etat for etat in etat_actif.transitions_without_action[\"targets\"] if etat not in etats_marques]\n",
    "                if etats_faisables != [] :\n",
    "                    pile.append(etats_faisables[0])\n",
    "                    etats_marques.append(etats_faisables[0])\n",
    "                else:\n",
    "                    pile.pop()\n",
    "            if pile == []:\n",
    "                S_int_2.remove(state)\n",
    "                S0.append(state) # on ajoute l'état à S0 si on ne peut pas atteindre un état de S1\n",
    "\n",
    "        print(f\"Les états S_0 sont : {S0}\")\n",
    "        print(f\"Les états S_? sont : {S_int_2}\")\n",
    "        print(f\"Les états S_1 sont : {S1}\")\n",
    "\n",
    "        A = np.zeros((len(S_int_2),len(S_int_2))) # Création de la matrice A\n",
    "        for i in range(len(S_int_2)):\n",
    "            etat_actif = self.states[S_int_2[i]]\n",
    "            for j in range(len(S_int_2)):\n",
    "                if S_int_2[j] in etat_actif.transitions_without_action[\"targets\"]:\n",
    "                    index = etat_actif.transitions_without_action[\"targets\"].index(S_int_2[j])\n",
    "                    A[i,j] = etat_actif.transitions_without_action[\"weights\"][index]/(np.sum(etat_actif.transitions_without_action[\"weights\"])) # on remplit la matrice A avec les bons poids\n",
    "\n",
    "        b = np.zeros((len(S_int_2),1)) # Création du vecteur b\n",
    "        for i in range(len(S_int_2)):\n",
    "            etat_actif = self.states[S_int_2[i]]\n",
    "            for transitions in etat_actif.transitions_without_action[\"targets\"]:\n",
    "                if transitions in S1:\n",
    "                    index = etat_actif.transitions_without_action[\"targets\"].index(transitions)\n",
    "                    b[i,0] += etat_actif.transitions_without_action[\"weights\"][index]/(np.sum(etat_actif.transitions_without_action[\"weights\"])) # on remplit le vecteur b avec les bons poids\n",
    "\n",
    "        # On résout (Id-A)^-1 * b\n",
    "        Id = np.identity(len(S_int_2))\n",
    "        y = np.dot(np.linalg.inv(Id - A),b)\n",
    "\n",
    "        print(f\"La probabilité de respecter la propriété à partir de l'état initial {self.liste_states[0]} est : {y[0,0]}\")\n",
    "        print(\" vecteur y total :\")\n",
    "        print(y)\n",
    "\n",
    "    def smc_quantitatif(self):\n",
    "        print(\"Quel sont les états cibles ? Veuillez les rentrer un par un, et taper 'fin' quand vous avez fini\")\n",
    "        time.sleep(0.5)\n",
    "        etat = input()\n",
    "        etats_cibles = []\n",
    "        while etat != \"fin\":\n",
    "            if etat not in self.liste_states:\n",
    "                print(\"L'état\", etat, \"n'existe pas\")\n",
    "                etat = input()\n",
    "            else :\n",
    "                etats_cibles.append(etat)\n",
    "                etat = input()\n",
    "        print(\"\\nPrécision souhaitée :\")\n",
    "        time.sleep(0.5)\n",
    "        precision = float(input())\n",
    "        print(\"\\nErreur souhaitée :\")\n",
    "        time.sleep(0.5)\n",
    "        erreur = float(input())\n",
    "        print(\"\\nLongueur des chaines voulues :\")\n",
    "        time.sleep(0.5)\n",
    "        longueur = int(input())\n",
    "\n",
    "        # Calcul de N (nombre d'itérations)\n",
    "        N =int((np.log(2)-np.log(erreur))/(2*precision)**2)+1\n",
    "\n",
    "        print(\"\\n----------------------------------\")\n",
    "        print(\"Récapitulatif des paramètres :\")\n",
    "        print(\"N = \", N, \"\\n\")\n",
    "        print(\"Precision (epsilon) = \", precision, \"\\n\")\n",
    "        print(\"Erreur (delta) = \", erreur, \"\\n\")\n",
    "        print(\"Longueur des chaines = \", longueur, \"\\n\")\n",
    "        print(\"----------------------------------\")\n",
    "        print(\"Calcul en cours.. \\n\")\n",
    "        # On lance un certain nombre d'itération pour chaque état cible\n",
    "        res_prob = [] # vecteur des probabilités d'atteindre les étates\n",
    "        for i in range(len(etats_cibles)): \n",
    "            lancers = [markov.parcours_SMC(self, longueur, etats_cibles[i]) for _ in range(N)]\n",
    "            res_prob.append(round(sum([1 for x in lancers if x==etats_cibles[i]])/N,5))\n",
    "        print(f\"Les probabilité d'atteindre chacun des états cibles {etats_cibles} sont : {res_prob}\")\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"States : {self.states} \\n Actions : {self.actions}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mettre random directement dans parcours (le choix sera fait par un input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: ['S0', 'S1', 'S2', 'S3', 'S4']\n",
      "Actions: ['None']\n",
      "Transition from S0 with no action and targets ['S1', 'S2'] with weights [5, 5]\n",
      "Transition from S1 with no action and targets ['S0', 'S3'] with weights [5, 5]\n",
      "Transition from S3 with no action and targets ['S3'] with weights [10]\n",
      "Transition from S2 with no action and targets ['S4'] with weights [10]\n",
      "Transition from S4 with no action and targets ['S1'] with weights [10]\n",
      "{'States': ['S0', 'S1', 'S2', 'S3', 'S4'], 'Actions': ['None'], 'Transitions_with_action': {}, 'Transitions_without_action': {0: {'from': 'S0', 'targets': ['S1', 'S2'], 'weights': [5, 5]}, 1: {'from': 'S1', 'targets': ['S0', 'S3'], 'weights': [5, 5]}, 2: {'from': 'S3', 'targets': ['S3'], 'weights': [10]}, 3: {'from': 'S2', 'targets': ['S4'], 'weights': [10]}, 4: {'from': 'S4', 'targets': ['S1'], 'weights': [10]}}}\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      " Souhaitez-vous que les problèmes détéctés prochainement soient résolus automatiquement si cela est possible? (y/n)\n",
      "Souhaitez-vous ajouter des récompenses aux états ? (y/n)\n",
      "Que souhaitez-vous faire ?\n",
      "1. lancer un parcours\n",
      "2. model checking d'un eventually\n",
      "3. SMC quantitatif\n",
      "Quel sont les états cibles ? Veuillez les rentrer un par un, et taper 'fin' quand vous avez fini\n",
      "\n",
      "Précision souhaitée :\n",
      "\n",
      "Erreur souhaitée :\n",
      "\n",
      "Longueur des chaines voulues :\n",
      "\n",
      "----------------------------------\n",
      "Récapitulatif des paramètres :\n",
      "N =  9223 \n",
      "\n",
      "Precision (epsilon) =  0.01 \n",
      "\n",
      "Erreur (delta) =  0.05 \n",
      "\n",
      "Longueur des chaines =  10 \n",
      "\n",
      "----------------------------------\n",
      "Calcul en cours.. \n",
      "\n",
      "Les probabilité d'atteindre chacun des états cibles ['S2'] sont : [0.6693]\n"
     ]
    }
   ],
   "source": [
    "M = markov(fichier_mdp=\"chaines/chaine_1.mdp\")\n",
    "# M.eventually_check_DTMC([\"S3\"])\n",
    "# M.simulation_random(10, mode_adv = \"random\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TP1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "40f7c2dd250ea64d1f119704b8cfa97c376b5923eda5d57d2493e68729089b32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
